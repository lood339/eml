{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9436e700-2949-4d9a-be9b-1ade02a0c224",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This colab notebook provides code and a framework for Lab 4: LLM Quantization. You will learn how to quantize a large language model that can run efficiently. We will implement AWQ (activation aware weight only quantization) for 4 bit weight-only quantization.\n",
    "\n",
    "Running large language models (LLMs) on the edge is of great importance, which not only enhances user experience but also addresses privacy concerns, as sensitive data remains localized and reduces the risk of potential breaches.\n",
    "\n",
    "However, deploying LLMs on the edge presents significant challenges. Edge devices operate under tight power constraints, setting them apart from workstations or cloud servers. This translates to restricted memory bandwidth and limited peak computation throughput on the edge. For instance, the NVIDIA Jetson Orin Nano, with its 8GB DRAM, cannot accommodate even the most compact LLaMA-2 model in half precision. Thankfully, AWQ presents a push-the-button solution for weight quantization, empowering LLM inference on edge devices with constrained memory.\n",
    "\n",
    "Furthermore, by using the AWQ 4-bit weight-only quantization algorithm, combined with an efficient 4-bit kernel, we can achieve the following acceleration on the RTX 4090. In the next lab section, we will also use TinyChatEnigne to achieve actual performance acceleration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78dd9c12-9119-4878-b9f6-fcba21c6a787",
   "metadata": {},
   "source": [
    "# AWQ (activation aware weight only quantization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daefb214-a73d-43b4-a0b1-19f2c80febca",
   "metadata": {},
   "source": [
    "Large language models (LLMs) have shown excellent performance on various tasks, but the astronomical model size raises the hardware barrier for serving (memory size) and slows down token generation (memory bandwidth). LLM sizes and computation are increasing exponentially, while memory bandwidth is increasing slowly. This gap is a major bottleneck for LLMs. In this lab, we will explore the use of an novel quantization algorithm (AWQ) to reduce memory footprint of LLMs and achieve accelerations for inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9fa099d-3444-4941-9dc2-8d757af05500",
   "metadata": {},
   "source": [
    "In previous courses, we have learned the basic methods of quantization.\n",
    "There are two types of quantization:\n",
    "\n",
    "- Quantize both weight and activation\n",
    "    - Better for computation-bounded scenarios: context stage, large batch inference\n",
    "    - For example, SmoothQuant: W8A8 quantization\n",
    "- Weight-only quantization\n",
    "    - Better for memory-bounded scenarios: decoding stage, single batch inference\n",
    "    - For example, AWQ that will be introduced in this lab: W4A16 quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a4e08c-cadb-48fa-a80c-eb2a4fe049c8",
   "metadata": {},
   "source": [
    "For the LLaMA-65B model, in the decoding stage of single batch inference, we need to perform GEMV $[1, 8192] \\times [8192, 8192]$. Taking the NVIDIA A100 80G as an example, its half-precision (FP16) performance is 312TFLOPS, and the memory bandwidth is about 2000GB/s. Therefore, its computation intensity is:\n",
    "\n",
    "$$\n",
    "\\frac{\\text{FLOP}}{\\text{Byte}} = \\frac{2\\times 8192^2}{8192^2} << \\frac{3.12\\times 10^{11}}{2\\times 10^9}\n",
    "$$\n",
    "\n",
    "This is very memory-bounded (~$10^2$ gap), which is why we need low-bit weight quantization. Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20ae47b-afbe-48af-b692-f5eb42b6ae5d",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9daa78b1-a2b1-485b-af2a-c7e725b4941a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from functools import partial\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938126b9-c368-48e6-8369-585cef269dfe",
   "metadata": {},
   "source": [
    "Here we use wikitext-2 dataset for evaluation. The dataset is automatically downloaded by the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9c12c7b-4169-49bf-9284-00742a2e8e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, tokenizer):\n",
    "    testenc = load_dataset('wikitext', 'wikitext-2-raw-v1', split='test')\n",
    "    testenc = tokenizer(\"\\n\\n\".join(testenc['text']), return_tensors='pt')\n",
    "\n",
    "    testenc = testenc.input_ids.to(model.device)\n",
    "    nsamples = 40\n",
    "    model = model.eval()\n",
    "\n",
    "    nlls = []\n",
    "    for i in tqdm.tqdm(range(nsamples), desc=\"evaluating...\"):\n",
    "        batch = testenc[:, (i * 2048):((i + 1) * 2048)].to(model.device)\n",
    "        with torch.no_grad():\n",
    "            lm_logits = model(batch).logits\n",
    "        shift_logits = lm_logits[:, :-1, :].contiguous().float()\n",
    "        shift_labels = testenc[:, (i * 2048):((i + 1) * 2048)][:, 1:]\n",
    "        loss_fct = nn.CrossEntropyLoss()\n",
    "        loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "        neg_log_likelihood = loss.float() * 2048\n",
    "        nlls.append(neg_log_likelihood)\n",
    "\n",
    "    return torch.exp(torch.stack(nlls).sum() / (nsamples * 2048))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "564edf89-4876-497e-9c6f-3da6965cda81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_size(model: nn.Module, data_width=16, group_size=-1):\n",
    "\n",
    "    if group_size != -1:\n",
    "        data_width += (16 + 4) / group_size\n",
    "\n",
    "    num_elements = 0\n",
    "    for param in model.parameters():\n",
    "        num_elements += param.numel()\n",
    "    return num_elements * data_width\n",
    "\n",
    "Byte = 8\n",
    "KiB = 1024 * Byte\n",
    "MiB = 1024 * KiB\n",
    "GiB = 1024 * MiB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6690a713-9047-4e0d-89f1-d77f34a9945e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68ffa85-d860-443e-b178-5a3404351748",
   "metadata": {},
   "source": [
    "Let's first evaluate the perplexity and model size of the FP32 Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6abede0-0dac-49b7-a846-b9bfe31a58c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"facebook/opt-1.3b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c74ecfe-f6eb-4ade-acb1-8187258566f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01b454ff15274ad1ae8590e5b4955471",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/2.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21eb0a182d1843659092dddbe3944eba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7174d6d0919e43b19ca561c23a778110",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4785199456904b9c95b39dc95aa44f97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1cfc14d6acb4ca187c86110a4b020f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/733k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5487d2abd1fe490fa9fd6150e97abbee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/6.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "326406897c584682b8a0506272721c91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation-00000-of-00001.parquet:   0%|          | 0.00/657k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7489b934f74140fe9d3b385b087b6bfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/4358 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4e5253b23064ad386feff35fae317ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/36718 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "941b4c20267442aaa1a7d1c95f4f7dda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/3760 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Aluating...:   0%|                                                                                                                                             | 0/40 [00:00<?, ?it/s]\n",
      "\u001b[Aluating...:   2%|███▎                                                                                                                                 | 1/40 [00:08<05:28,  8.43s/it]\n",
      "\u001b[Aluating...:   5%|██████▋                                                                                                                              | 2/40 [00:16<05:05,  8.04s/it]\n",
      "\u001b[Aluating...:   8%|█████████▉                                                                                                                           | 3/40 [00:24<04:56,  8.01s/it]\n",
      "\u001b[Aluating...:  10%|█████████████▎                                                                                                                       | 4/40 [00:32<04:48,  8.01s/it]\n",
      "\u001b[Aluating...:  12%|████████████████▋                                                                                                                    | 5/40 [00:40<04:38,  7.97s/it]\n",
      "\u001b[Aluating...:  15%|███████████████████▉                                                                                                                 | 6/40 [00:48<04:31,  7.98s/it]\n",
      "\u001b[Aluating...:  18%|███████████████████████▎                                                                                                             | 7/40 [00:55<04:18,  7.85s/it]\n",
      "\u001b[Aluating...:  20%|██████████████████████████▌                                                                                                          | 8/40 [01:03<04:09,  7.80s/it]\n",
      "\u001b[Aluating...:  22%|█████████████████████████████▉                                                                                                       | 9/40 [01:11<04:08,  8.00s/it]\n",
      "\u001b[Aluating...:  25%|█████████████████████████████████                                                                                                   | 10/40 [01:19<03:56,  7.88s/it]\n",
      "\u001b[Aluating...:  28%|████████████████████████████████████▎                                                                                               | 11/40 [01:26<03:44,  7.72s/it]\n",
      "\u001b[Aluating...:  30%|███████████████████████████████████████▌                                                                                            | 12/40 [01:34<03:35,  7.68s/it]\n",
      "\u001b[Aluating...:  32%|██████████████████████████████████████████▉                                                                                         | 13/40 [01:41<03:25,  7.61s/it]\n",
      "\u001b[Aluating...:  35%|██████████████████████████████████████████████▏                                                                                     | 14/40 [01:49<03:17,  7.60s/it]\n",
      "\u001b[Aluating...:  38%|█████████████████████████████████████████████████▌                                                                                  | 15/40 [01:56<03:08,  7.53s/it]\n",
      "\u001b[Aluating...:  40%|████████████████████████████████████████████████████▊                                                                               | 16/40 [02:04<02:59,  7.47s/it]\n",
      "\u001b[Aluating...:  42%|████████████████████████████████████████████████████████                                                                            | 17/40 [02:11<02:52,  7.48s/it]\n",
      "\u001b[Aluating...:  45%|███████████████████████████████████████████████████████████▍                                                                        | 18/40 [02:18<02:43,  7.44s/it]\n",
      "\u001b[Aluating...:  48%|██████████████████████████████████████████████████████████████▋                                                                     | 19/40 [02:26<02:37,  7.48s/it]\n",
      "\u001b[Aluating...:  50%|██████████████████████████████████████████████████████████████████                                                                  | 20/40 [02:34<02:30,  7.53s/it]\n",
      "\u001b[Aluating...:  52%|█████████████████████████████████████████████████████████████████████▎                                                              | 21/40 [02:41<02:23,  7.56s/it]\n",
      "\u001b[Aluating...:  55%|████████████████████████████████████████████████████████████████████████▌                                                           | 22/40 [02:49<02:16,  7.57s/it]\n",
      "\u001b[Aluating...:  57%|███████████████████████████████████████████████████████████████████████████▉                                                        | 23/40 [02:56<02:08,  7.54s/it]\n",
      "\u001b[Aluating...:  60%|███████████████████████████████████████████████████████████████████████████████▏                                                    | 24/40 [03:05<02:03,  7.74s/it]\n",
      "\u001b[Aluating...:  62%|██████████████████████████████████████████████████████████████████████████████████▌                                                 | 25/40 [03:12<01:55,  7.72s/it]\n",
      "\u001b[Aluating...:  65%|█████████████████████████████████████████████████████████████████████████████████████▊                                              | 26/40 [03:20<01:46,  7.60s/it]\n",
      "\u001b[Aluating...:  68%|█████████████████████████████████████████████████████████████████████████████████████████                                           | 27/40 [03:27<01:37,  7.52s/it]\n",
      "\u001b[Aluating...:  70%|████████████████████████████████████████████████████████████████████████████████████████████▍                                       | 28/40 [03:34<01:30,  7.52s/it]\n",
      "\u001b[Aluating...:  72%|███████████████████████████████████████████████████████████████████████████████████████████████▋                                    | 29/40 [03:44<01:28,  8.06s/it]\n",
      "\u001b[Aluating...:  75%|███████████████████████████████████████████████████████████████████████████████████████████████████                                 | 30/40 [03:54<01:26,  8.62s/it]\n",
      "\u001b[Aluating...:  78%|██████████████████████████████████████████████████████████████████████████████████████████████████████▎                             | 31/40 [04:03<01:20,  8.94s/it]\n",
      "\u001b[Aluating...:  80%|█████████████████████████████████████████████████████████████████████████████████████████████████████████▌                          | 32/40 [04:13<01:13,  9.20s/it]\n",
      "\u001b[Aluating...:  82%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                       | 33/40 [04:22<01:03,  9.12s/it]\n",
      "\u001b[Aluating...:  85%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                   | 34/40 [04:30<00:51,  8.63s/it]\n",
      "\u001b[Aluating...:  88%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                | 35/40 [04:37<00:40,  8.15s/it]\n",
      "\u001b[Aluating...:  90%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊             | 36/40 [04:44<00:31,  7.82s/it]\n",
      "\u001b[Aluating...:  92%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████          | 37/40 [04:51<00:22,  7.58s/it]\n",
      "\u001b[Aluating...:  95%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍      | 38/40 [04:58<00:14,  7.42s/it]\n",
      "\u001b[Aluating...:  98%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋   | 39/40 [05:05<00:07,  7.32s/it]\n",
      "evaluating...: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 40/40 [05:12<00:00,  7.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "model perplexity: 14.47\n",
      "model size: 5043.73 MiB\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "model_perplexity = evaluate(model, tokenizer)\n",
    "model_size = get_model_size(model, data_width=32, group_size=128)\n",
    "print(f\"\\nmodel perplexity: {model_perplexity:.2f}\")\n",
    "print(f\"model size: {model_size/MiB:.2f} MiB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d312951-8ded-4507-971d-1b881aaf18c6",
   "metadata": {},
   "source": [
    "model perplexity: 14.47, lower is better\n",
    "model size: 5043.73 MiB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e28264f-b975-4f72-859b-f5c0b1560287",
   "metadata": {},
   "source": [
    "### pseudo quantization\n",
    "The following code is for pseudo quantization.\n",
    "\n",
    "Pseudo Quantization is used to simulate the effects of quantization on a model  without actually quantizing the model's weights. (i.e. rounding to the nearest quantized value and then **dequantizing back to a float**.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d41cd2a-f280-4715-9edd-4742e2d19415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# core quantization method (simulated quantization)\n",
    "def pseudo_quantize_tensor(w, n_bit=4, q_group_size=-1):\n",
    "    w_shape = w.shape\n",
    "    if q_group_size > 0:\n",
    "        assert w_shape[-1] % q_group_size == 0\n",
    "        w = w.reshape(-1, q_group_size)\n",
    "    assert w.dim() == 2\n",
    "\n",
    "    # max-min values\n",
    "    max_v = w.amax(dim=1, keepdim=True)\n",
    "    assert max_v.dim() == 2 and max_v.size(0) == w.size(0) and max_v.size(1) == 1\n",
    "    min_v = w.amin(dim=1, keepdim=True)\n",
    "    assert min_v.dim() == 2 and min_v.size(0) == w.size(0) and min_v.size(1) == 1\n",
    "\n",
    "    # Calculate the scale factor and zero point.  (Formula 1 & 2)\n",
    "    max_int = 2 ** n_bit - 1\n",
    "    scales = (max_v - min_v).clamp(min=1e-5) / max_int\n",
    "    assert scales.shape == max_v.shape\n",
    "    zeros = (-torch.round(min_v/scales)).clamp_(0, max_int)\n",
    "    assert scales.shape == min_v.shape     \n",
    "    \n",
    "    assert torch.isnan(scales).sum() == 0\n",
    "    assert torch.isnan(w).sum() == 0\n",
    "\n",
    "    # Quantize W: Map values in the range [\\beta, \\alpha] to lie within [0, 2^b - 1] (Formula 3)\n",
    "    w = torch.clamp(torch.round(w/scales) + zeros, 0, max_int)\n",
    "    assert w.dim() == 2 and w.size(0) == scales.size(0) and w.size(1) == q_group_size\n",
    "\n",
    "    # Dequantize W (pseudo quantization, the inverse transformation of Formula 3)\n",
    "    w = (w-zeros) * scales\n",
    "    assert w.dim() == 2 and w.size(0) == scales.size(0) and w.size(1) == q_group_size\n",
    "    \n",
    "    assert torch.isnan(w).sum() == 0\n",
    "\n",
    "    w = w.reshape(w_shape)\n",
    "    return w\n",
    "\n",
    "@torch.no_grad()\n",
    "def pseudo_quantize_model_weight(\n",
    "    model, w_bit, q_group_size\n",
    "):\n",
    "    for n, m in model.named_modules():\n",
    "        if isinstance(m, nn.Linear):\n",
    "            m.weight.data = pseudo_quantize_tensor(m.weight.data, n_bit=w_bit, q_group_size=q_group_size)   \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5264c5fb-b47e-4f99-8f7f-6219769335d7",
   "metadata": {},
   "source": [
    "Let's evaluate the perplexity and model size of the quantized 3-bit Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0131195f-8861-40b9-9dc2-ba611a8df641",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating...: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 40/40 [04:59<00:00,  7.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "model perplexity: 123.78\n",
      "model size: 495.06 MiB\n"
     ]
    }
   ],
   "source": [
    "del model\n",
    "gc.collect()\n",
    "torch.mps.empty_cache()\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\", offload_folder=\"offload_weights\") # offload_folder=\"offload_weights\"\n",
    "#model.to(device)\n",
    "pseudo_quantize_model_weight(model, w_bit=3, q_group_size=128)\n",
    "\n",
    "# Evaluate the model\n",
    "model_perplexity = evaluate(model, tokenizer)\n",
    "model_size = get_model_size(model, data_width=3, group_size=128)\n",
    "print(f\"\\nmodel perplexity: {model_perplexity:.2f}\")\n",
    "print(f\"model size: {model_size/MiB:.2f} MiB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909aab89-5628-49c9-b84b-a4c8b4d7ccc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model perplexity: 123.78\n",
    "model size: 495.06 MiB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b7b76d-506c-45b0-a734-5490d27900e8",
   "metadata": {},
   "source": [
    "We can see that the model size has decreased, but the perplexity has significantly increased."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b12d660-9a1c-4c23-8322-1a267685686d",
   "metadata": {},
   "source": [
    "There is an observation in LLM activations that **outliers appear in a small fraction of the channels**. If one channel has an outlier, it **persistently appears in all tokens**. The variance amongst the channels for a given token is large (the activations in some channels are very large, but most are small), but the variance between the magnitudes of a given channel across tokens is small (outlier channels are consistently large).\n",
    "\n",
    "According to the observation of AWQ, weight channels corresponding to activation outliers are more salient, and preserving those salient weights can lead to a significant performance improvement. Next, let's try to find the salient weights and retain them as original values to observe the change in perplexity.\n",
    "\n",
    "The following code is used to load the calibration dataset, so as to obtain activation outliers to identify salient weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2567ca17-d3ea-46d4-8d9b-a3fed6f98d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_calib_dataset(tokenizer=None, n_samples=256, block_size=512):\n",
    "    dataset = load_dataset(\"mit-han-lab/pile-val-backup\", split=\"validation\")\n",
    "    dataset = dataset.shuffle(seed=42)\n",
    "    samples, n_run = [], 0\n",
    "    for data in dataset:\n",
    "        line = data['text']\n",
    "        line = line.strip()\n",
    "        line_encoded = tokenizer.encode(line)\n",
    "        if len(line_encoded) > block_size:\n",
    "            continue\n",
    "        sample = torch.tensor([line_encoded])\n",
    "        if sample.numel() == 0:\n",
    "            continue\n",
    "        samples.append(sample)\n",
    "        n_run += 1\n",
    "        if n_run == n_samples:\n",
    "            break\n",
    "    # concatenate all samples and split them by block size\n",
    "    cat_samples = torch.cat(samples, dim=1)\n",
    "    n_split = cat_samples.shape[1] // block_size\n",
    "    print(f\" * Split into {n_split} blocks\")\n",
    "    return [cat_samples[:, i*block_size:(i+1)*block_size] for i in range(n_split)]\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_calib_activation(model, tokenizer):\n",
    "    input_dict = dict()\n",
    "\n",
    "    def stat_input_max_hook(m, x, y, name):\n",
    "        if isinstance(x, tuple):\n",
    "            x = x[0]\n",
    "        x_max = x.view(-1, x.shape[-1]).abs().mean(dim=0).cpu().detach()\n",
    "        if name not in input_dict:\n",
    "            input_dict[name] = [x_max]\n",
    "        else:\n",
    "            input_dict[name] += [x_max]\n",
    "\n",
    "    hooks = []\n",
    "    for name, m in model.named_modules():\n",
    "        if isinstance(m, nn.Linear):\n",
    "            hooks.append(m.register_forward_hook(partial(stat_input_max_hook, name=name)))\n",
    "\n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    print(f\"device: {device}\")\n",
    "\n",
    "    samples = get_calib_dataset(tokenizer)\n",
    "    pbar = tqdm.tqdm(samples)\n",
    "    for input_ids in pbar:\n",
    "        input_ids = input_ids.to(device)\n",
    "        model(input_ids)\n",
    "\n",
    "    for hook in hooks:\n",
    "        hook.remove()\n",
    "\n",
    "    return input_dict\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9984ddb2-483c-4c5b-a2bd-b41fb1a99d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f685266ac06c456aa2c78507e312dc9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/214670 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Split into 127 blocks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|████████████████▏                                                                                                                                  | 14/127 [00:22<02:56,  1.56s/it]"
     ]
    }
   ],
   "source": [
    "del model\n",
    "gc.collect()\n",
    "torch.mps.empty_cache()\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\", offload_folder=\"offload_weights\")\n",
    "input_feat = get_calib_activation(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5f0da38a-30b4-4ded-b209-18af52fc6d7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "814a8928eaf9423fadc76841f89e5920",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/214670 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "DatasetGenerationError",
     "evalue": "An error occurred while generating the dataset",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/opt/miniconda3/envs/eml/lib/python3.12/site-packages/datasets/builder.py:1855\u001b[0m, in \u001b[0;36mArrowBasedBuilder._prepare_split_single\u001b[0;34m(self, gen_kwargs, fpath, file_format, max_shard_size, job_id)\u001b[0m\n\u001b[1;32m   1854\u001b[0m _time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m-> 1855\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1856\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmax_shard_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mwriter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_bytes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmax_shard_size\u001b[49m\u001b[43m:\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/eml/lib/python3.12/site-packages/datasets/packaged_modules/json/json.py:99\u001b[0m, in \u001b[0;36mJson._generate_tables\u001b[0;34m(self, files)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_generate_tables\u001b[39m(\u001b[38;5;28mself\u001b[39m, files):\n\u001b[0;32m---> 99\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfile_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mitertools\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_iterable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfiles\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# If the file is one json object and if we need to look at the items in one specific field\u001b[39;49;00m\n\u001b[1;32m    101\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfield\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m:\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/eml/lib/python3.12/site-packages/datasets/utils/track.py:49\u001b[0m, in \u001b[0;36mTrackedIterableFromGenerator.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 49\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlast_item\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/eml/lib/python3.12/site-packages/datasets/utils/file_utils.py:1366\u001b[0m, in \u001b[0;36mFilesIterable._iter_from_urlpaths\u001b[0;34m(cls, urlpaths, download_config)\u001b[0m\n\u001b[1;32m   1365\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m urlpath\n\u001b[0;32m-> 1366\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[43mxisdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43murlpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1367\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m dirpath, dirnames, filenames \u001b[38;5;129;01min\u001b[39;00m xwalk(urlpath, download_config\u001b[38;5;241m=\u001b[39mdownload_config):\n\u001b[1;32m   1368\u001b[0m         \u001b[38;5;66;03m# in-place modification to prune the search\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/eml/lib/python3.12/site-packages/datasets/utils/file_utils.py:799\u001b[0m, in \u001b[0;36mxisdir\u001b[0;34m(path, download_config)\u001b[0m\n\u001b[1;32m    798\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 799\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43minner_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/eml/lib/python3.12/site-packages/fsspec/spec.py:701\u001b[0m, in \u001b[0;36mAbstractFileSystem.isdir\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    700\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 701\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdirectory\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/eml/lib/python3.12/site-packages/fsspec/archive.py:38\u001b[0m, in \u001b[0;36mAbstractArchiveFileSystem.info\u001b[0;34m(self, path, **kwargs)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minfo\u001b[39m(\u001b[38;5;28mself\u001b[39m, path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 38\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_dirs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m     path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_strip_protocol(path)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/eml/lib/python3.12/site-packages/datasets/filesystems/compression.py:66\u001b[0m, in \u001b[0;36mBaseCompressedFileFileSystem._get_dirs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdir_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 66\u001b[0m     f \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open_with_fsspec\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mfs\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfo), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muncompressed_name}\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdir_cache \u001b[38;5;241m=\u001b[39m {f[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m]: f}\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/eml/lib/python3.12/site-packages/fsspec/core.py:491\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(urlpath, mode, compression, encoding, errors, protocol, newline, expand, **kwargs)\u001b[0m\n\u001b[1;32m    490\u001b[0m expand \u001b[38;5;241m=\u001b[39m DEFAULT_EXPAND \u001b[38;5;28;01mif\u001b[39;00m expand \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m expand\n\u001b[0;32m--> 491\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mopen_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m    \u001b[49m\u001b[43murlpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43murlpath\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprotocol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnewline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexpand\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpand\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m out:\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/eml/lib/python3.12/site-packages/fsspec/core.py:315\u001b[0m, in \u001b[0;36mopen_files\u001b[0;34m(urlpath, mode, compression, encoding, errors, name_function, num, protocol, newline, auto_mkdir, expand, **kwargs)\u001b[0m\n\u001b[1;32m    312\u001b[0m             \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m OpenFiles(\n\u001b[1;32m    314\u001b[0m     [\n\u001b[0;32m--> 315\u001b[0m         \u001b[43mOpenFile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnewline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    324\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m paths\n\u001b[1;32m    325\u001b[0m     ],\n\u001b[1;32m    326\u001b[0m     mode\u001b[38;5;241m=\u001b[39mmode,\n\u001b[1;32m    327\u001b[0m     fs\u001b[38;5;241m=\u001b[39mfs,\n\u001b[1;32m    328\u001b[0m )\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/eml/lib/python3.12/site-packages/fsspec/core.py:78\u001b[0m, in \u001b[0;36mOpenFile.__init__\u001b[0;34m(self, fs, path, mode, compression, encoding, errors, newline)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m=\u001b[39m mode\n\u001b[0;32m---> 78\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompression \u001b[38;5;241m=\u001b[39m \u001b[43mget_compression\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;241m=\u001b[39m encoding\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/eml/lib/python3.12/site-packages/fsspec/core.py:544\u001b[0m, in \u001b[0;36mget_compression\u001b[0;34m(urlpath, compression)\u001b[0m\n\u001b[1;32m    543\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m compression \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m compr:\n\u001b[0;32m--> 544\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompression type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcompression\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not supported\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    545\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m compression\n",
      "\u001b[0;31mValueError\u001b[0m: Compression type zstd not supported",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mDatasetGenerationError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmit-han-lab/pile-val-backup\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalidation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/eml/lib/python3.12/site-packages/datasets/load.py:2083\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001b[0m\n\u001b[1;32m   2080\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m builder_instance\u001b[38;5;241m.\u001b[39mas_streaming_dataset(split\u001b[38;5;241m=\u001b[39msplit)\n\u001b[1;32m   2082\u001b[0m \u001b[38;5;66;03m# Download and prepare data\u001b[39;00m\n\u001b[0;32m-> 2083\u001b[0m \u001b[43mbuilder_instance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2084\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2085\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2086\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2087\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2088\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2089\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2091\u001b[0m \u001b[38;5;66;03m# Build dataset for splits\u001b[39;00m\n\u001b[1;32m   2092\u001b[0m keep_in_memory \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2093\u001b[0m     keep_in_memory \u001b[38;5;28;01mif\u001b[39;00m keep_in_memory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m is_small_dataset(builder_instance\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdataset_size)\n\u001b[1;32m   2094\u001b[0m )\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/eml/lib/python3.12/site-packages/datasets/builder.py:925\u001b[0m, in \u001b[0;36mDatasetBuilder.download_and_prepare\u001b[0;34m(self, output_dir, download_config, download_mode, verification_mode, dl_manager, base_path, file_format, max_shard_size, num_proc, storage_options, **download_and_prepare_kwargs)\u001b[0m\n\u001b[1;32m    923\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_proc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    924\u001b[0m     prepare_split_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_proc\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_proc\n\u001b[0;32m--> 925\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_download_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdl_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    928\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mprepare_split_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    929\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdownload_and_prepare_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    930\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    931\u001b[0m \u001b[38;5;66;03m# Sync info\u001b[39;00m\n\u001b[1;32m    932\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdataset_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(split\u001b[38;5;241m.\u001b[39mnum_bytes \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39msplits\u001b[38;5;241m.\u001b[39mvalues())\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/eml/lib/python3.12/site-packages/datasets/builder.py:1001\u001b[0m, in \u001b[0;36mDatasetBuilder._download_and_prepare\u001b[0;34m(self, dl_manager, verification_mode, **prepare_split_kwargs)\u001b[0m\n\u001b[1;32m    997\u001b[0m split_dict\u001b[38;5;241m.\u001b[39madd(split_generator\u001b[38;5;241m.\u001b[39msplit_info)\n\u001b[1;32m    999\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1000\u001b[0m     \u001b[38;5;66;03m# Prepare split will record examples associated to the split\u001b[39;00m\n\u001b[0;32m-> 1001\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mprepare_split_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1002\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1003\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[1;32m   1004\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot find data file. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1005\u001b[0m         \u001b[38;5;241m+\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmanual_download_instructions \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1006\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mOriginal error:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1007\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)\n\u001b[1;32m   1008\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/eml/lib/python3.12/site-packages/datasets/builder.py:1742\u001b[0m, in \u001b[0;36mArrowBasedBuilder._prepare_split\u001b[0;34m(self, split_generator, file_format, num_proc, max_shard_size)\u001b[0m\n\u001b[1;32m   1740\u001b[0m job_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   1741\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pbar:\n\u001b[0;32m-> 1742\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mjob_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_split_single\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1743\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgen_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgen_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjob_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjob_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_prepare_split_args\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1745\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1746\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/eml/lib/python3.12/site-packages/datasets/builder.py:1898\u001b[0m, in \u001b[0;36mArrowBasedBuilder._prepare_split_single\u001b[0;34m(self, gen_kwargs, fpath, file_format, max_shard_size, job_id)\u001b[0m\n\u001b[1;32m   1896\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, DatasetGenerationError):\n\u001b[1;32m   1897\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1898\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DatasetGenerationError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while generating the dataset\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m   1900\u001b[0m \u001b[38;5;28;01myield\u001b[39;00m job_id, \u001b[38;5;28;01mTrue\u001b[39;00m, (total_num_examples, total_num_bytes, writer\u001b[38;5;241m.\u001b[39m_features, num_shards, shard_lengths)\n",
      "\u001b[0;31mDatasetGenerationError\u001b[0m: An error occurred while generating the dataset"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"mit-han-lab/pile-val-backup\", split=\"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "db3624e7-f8e0-480e-8d77-a11a703937ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256\n"
     ]
    }
   ],
   "source": [
    "print(torch.mps.current_allocated_memory())  # Current allocated memory\n",
    "#print(torch.mps.current_reserved_memory())   # Reserved memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "83c272bc-ac01-446e-a4af-71c41ee0cdfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.device'>\n"
     ]
    }
   ],
   "source": [
    "print(type(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1ddd398f-c3db-4bb2-a947-6b7bee0bace1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Example: Suppose `tensor` is on MPS\n",
    "tensor = torch.ones(1000, device=\"mps\")\n",
    "\n",
    "# Move to CPU before deleting\n",
    "tensor = tensor.to(\"cpu\")\n",
    "del tensor  # Delete tensor\n",
    "\n",
    "# Run garbage collection\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "# Empty MPS cache\n",
    "torch.mps.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "080cf20f-9a7f-4b44-beb7-c3e8351becc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function load_dataset in module datasets.load:\n",
      "\n",
      "load_dataset(path: str, name: Optional[str] = None, data_dir: Optional[str] = None, data_files: Union[str, collections.abc.Sequence[str], collections.abc.Mapping[str, Union[str, collections.abc.Sequence[str]]], NoneType] = None, split: Union[str, datasets.splits.Split, NoneType] = None, cache_dir: Optional[str] = None, features: Optional[datasets.features.features.Features] = None, download_config: Optional[datasets.download.download_config.DownloadConfig] = None, download_mode: Union[datasets.download.download_manager.DownloadMode, str, NoneType] = None, verification_mode: Union[datasets.utils.info_utils.VerificationMode, str, NoneType] = None, keep_in_memory: Optional[bool] = None, save_infos: bool = False, revision: Union[str, datasets.utils.version.Version, NoneType] = None, token: Union[bool, str, NoneType] = None, streaming: bool = False, num_proc: Optional[int] = None, storage_options: Optional[dict] = None, trust_remote_code: Optional[bool] = None, **config_kwargs) -> Union[datasets.dataset_dict.DatasetDict, datasets.arrow_dataset.Dataset, datasets.dataset_dict.IterableDatasetDict, datasets.iterable_dataset.IterableDataset]\n",
      "    Load a dataset from the Hugging Face Hub, or a local dataset.\n",
      "\n",
      "    You can find the list of datasets on the [Hub](https://huggingface.co/datasets) or with [`huggingface_hub.list_datasets`].\n",
      "\n",
      "    A dataset is a directory that contains some data files in generic formats (JSON, CSV, Parquet, etc.) and possibly\n",
      "    in a generic structure (Webdataset, ImageFolder, AudioFolder, VideoFolder, etc.)\n",
      "\n",
      "    This function does the following under the hood:\n",
      "\n",
      "        1. Load a dataset builder:\n",
      "\n",
      "            * Find the most common data format in the dataset and pick its associated builder (JSON, CSV, Parquet, Webdataset, ImageFolder, AudioFolder, etc.)\n",
      "            * Find which file goes into which split (e.g. train/test) based on file and directory names or on the YAML configuration\n",
      "            * It is also possible to specify `data_files` manually, and which dataset builder to use (e.g. \"parquet\").\n",
      "\n",
      "        2. Run the dataset builder:\n",
      "\n",
      "            In the general case:\n",
      "\n",
      "            * Download the data files from the dataset if they are not already available locally or cached.\n",
      "            * Process and cache the dataset in typed Arrow tables for caching.\n",
      "\n",
      "                Arrow table are arbitrarily long, typed tables which can store nested objects and be mapped to numpy/pandas/python generic types.\n",
      "                They can be directly accessed from disk, loaded in RAM or even streamed over the web.\n",
      "\n",
      "            In the streaming case:\n",
      "\n",
      "            * Don't download or cache anything. Instead, the dataset is lazily loaded and will be streamed on-the-fly when iterating on it.\n",
      "\n",
      "        3. Return a dataset built from the requested splits in `split` (default: all).\n",
      "\n",
      "    It can also use a custom dataset builder if the dataset contains a dataset script, but this feature is mostly for backward compatibility.\n",
      "    In this case the dataset script file must be named after the dataset repository or directory and end with \".py\".\n",
      "\n",
      "    Args:\n",
      "\n",
      "        path (`str`):\n",
      "            Path or name of the dataset.\n",
      "\n",
      "            - if `path` is a dataset repository on the HF hub (list all available datasets with [`huggingface_hub.list_datasets`])\n",
      "              -> load the dataset from supported files in the repository (csv, json, parquet, etc.)\n",
      "              e.g. `'username/dataset_name'`, a dataset repository on the HF hub containing the data files.\n",
      "\n",
      "            - if `path` is a local directory\n",
      "              -> load the dataset from supported files in the directory (csv, json, parquet, etc.)\n",
      "              e.g. `'./path/to/directory/with/my/csv/data'`.\n",
      "\n",
      "            - if `path` is the name of a dataset builder and `data_files` or `data_dir` is specified\n",
      "              (available builders are \"json\", \"csv\", \"parquet\", \"arrow\", \"text\", \"xml\", \"webdataset\", \"imagefolder\", \"audiofolder\", \"videofolder\")\n",
      "              -> load the dataset from the files in `data_files` or `data_dir`\n",
      "              e.g. `'parquet'`.\n",
      "\n",
      "            It can also point to a local dataset script but this is not recommended.\n",
      "        name (`str`, *optional*):\n",
      "            Defining the name of the dataset configuration.\n",
      "        data_dir (`str`, *optional*):\n",
      "            Defining the `data_dir` of the dataset configuration. If specified for the generic builders (csv, text etc.) or the Hub datasets and `data_files` is `None`,\n",
      "            the behavior is equal to passing `os.path.join(data_dir, **)` as `data_files` to reference all the files in a directory.\n",
      "        data_files (`str` or `Sequence` or `Mapping`, *optional*):\n",
      "            Path(s) to source data file(s).\n",
      "        split (`Split` or `str`):\n",
      "            Which split of the data to load.\n",
      "            If `None`, will return a `dict` with all splits (typically `datasets.Split.TRAIN` and `datasets.Split.TEST`).\n",
      "            If given, will return a single Dataset.\n",
      "            Splits can be combined and specified like in tensorflow-datasets.\n",
      "        cache_dir (`str`, *optional*):\n",
      "            Directory to read/write data. Defaults to `\"~/.cache/huggingface/datasets\"`.\n",
      "        features (`Features`, *optional*):\n",
      "            Set the features type to use for this dataset.\n",
      "        download_config ([`DownloadConfig`], *optional*):\n",
      "            Specific download configuration parameters.\n",
      "        download_mode ([`DownloadMode`] or `str`, defaults to `REUSE_DATASET_IF_EXISTS`):\n",
      "            Download/generate mode.\n",
      "        verification_mode ([`VerificationMode`] or `str`, defaults to `BASIC_CHECKS`):\n",
      "            Verification mode determining the checks to run on the downloaded/processed dataset information (checksums/size/splits/...).\n",
      "\n",
      "            <Added version=\"2.9.1\"/>\n",
      "        keep_in_memory (`bool`, defaults to `None`):\n",
      "            Whether to copy the dataset in-memory. If `None`, the dataset\n",
      "            will not be copied in-memory unless explicitly enabled by setting `datasets.config.IN_MEMORY_MAX_SIZE` to\n",
      "            nonzero. See more details in the [improve performance](../cache#improve-performance) section.\n",
      "        save_infos (`bool`, defaults to `False`):\n",
      "            Save the dataset information (checksums/size/splits/...).\n",
      "        revision ([`Version`] or `str`, *optional*):\n",
      "            Version of the dataset script to load.\n",
      "            As datasets have their own git repository on the Datasets Hub, the default version \"main\" corresponds to their \"main\" branch.\n",
      "            You can specify a different version than the default \"main\" by using a commit SHA or a git tag of the dataset repository.\n",
      "        token (`str` or `bool`, *optional*):\n",
      "            Optional string or boolean to use as Bearer token for remote files on the Datasets Hub.\n",
      "            If `True`, or not specified, will get token from `\"~/.huggingface\"`.\n",
      "        streaming (`bool`, defaults to `False`):\n",
      "            If set to `True`, don't download the data files. Instead, it streams the data progressively while\n",
      "            iterating on the dataset. An [`IterableDataset`] or [`IterableDatasetDict`] is returned instead in this case.\n",
      "\n",
      "            Note that streaming works for datasets that use data formats that support being iterated over like txt, csv, jsonl for example.\n",
      "            Json files may be downloaded completely. Also streaming from remote zip or gzip files is supported but other compressed formats\n",
      "            like rar and xz are not yet supported. The tgz format doesn't allow streaming.\n",
      "        num_proc (`int`, *optional*, defaults to `None`):\n",
      "            Number of processes when downloading and generating the dataset locally.\n",
      "            Multiprocessing is disabled by default.\n",
      "\n",
      "            <Added version=\"2.7.0\"/>\n",
      "        storage_options (`dict`, *optional*, defaults to `None`):\n",
      "            **Experimental**. Key/value pairs to be passed on to the dataset file-system backend, if any.\n",
      "\n",
      "            <Added version=\"2.11.0\"/>\n",
      "        trust_remote_code (`bool`, *optional*, defaults to `None`):\n",
      "            Whether or not to allow for datasets defined on the Hub using a dataset script. This option\n",
      "            should only be set to `True` for repositories you trust and in which you have read the code, as it will\n",
      "            execute code present on the Hub on your local machine.\n",
      "\n",
      "            <Added version=\"2.16.0\"/>\n",
      "\n",
      "            <Changed version=\"2.20.0\">\n",
      "\n",
      "            `trust_remote_code` defaults to `False` if not specified.\n",
      "\n",
      "            </Changed>\n",
      "\n",
      "        **config_kwargs (additional keyword arguments):\n",
      "            Keyword arguments to be passed to the `BuilderConfig`\n",
      "            and used in the [`DatasetBuilder`].\n",
      "\n",
      "    Returns:\n",
      "        [`Dataset`] or [`DatasetDict`]:\n",
      "        - if `split` is not `None`: the dataset requested,\n",
      "        - if `split` is `None`, a [`~datasets.DatasetDict`] with each split.\n",
      "\n",
      "        or [`IterableDataset`] or [`IterableDatasetDict`]: if `streaming=True`\n",
      "\n",
      "        - if `split` is not `None`, the dataset is requested\n",
      "        - if `split` is `None`, a [`~datasets.streaming.IterableDatasetDict`] with each split.\n",
      "\n",
      "    Example:\n",
      "\n",
      "    Load a dataset from the Hugging Face Hub:\n",
      "\n",
      "    ```py\n",
      "    >>> from datasets import load_dataset\n",
      "    >>> ds = load_dataset('cornell-movie-review-data/rotten_tomatoes', split='train')\n",
      "\n",
      "    # Load a subset or dataset configuration (here 'sst2')\n",
      "    >>> from datasets import load_dataset\n",
      "    >>> ds = load_dataset('nyu-mll/glue', 'sst2', split='train')\n",
      "\n",
      "    # Manual mapping of data files to splits\n",
      "    >>> data_files = {'train': 'train.csv', 'test': 'test.csv'}\n",
      "    >>> ds = load_dataset('namespace/your_dataset_name', data_files=data_files)\n",
      "\n",
      "    # Manual selection of a directory to load\n",
      "    >>> ds = load_dataset('namespace/your_dataset_name', data_dir='folder_name')\n",
      "    ```\n",
      "\n",
      "    Load a local dataset:\n",
      "\n",
      "    ```py\n",
      "    # Load a CSV file\n",
      "    >>> from datasets import load_dataset\n",
      "    >>> ds = load_dataset('csv', data_files='path/to/local/my_dataset.csv')\n",
      "\n",
      "    # Load a JSON file\n",
      "    >>> from datasets import load_dataset\n",
      "    >>> ds = load_dataset('json', data_files='path/to/local/my_dataset.json')\n",
      "\n",
      "    # Load from a local loading script (not recommended)\n",
      "    >>> from datasets import load_dataset\n",
      "    >>> ds = load_dataset('path/to/local/loading_script/loading_script.py', split='train')\n",
      "    ```\n",
      "\n",
      "    Load an [`~datasets.IterableDataset`]:\n",
      "\n",
      "    ```py\n",
      "    >>> from datasets import load_dataset\n",
      "    >>> ds = load_dataset('cornell-movie-review-data/rotten_tomatoes', split='train', streaming=True)\n",
      "    ```\n",
      "\n",
      "    Load an image dataset with the `ImageFolder` dataset builder:\n",
      "\n",
      "    ```py\n",
      "    >>> from datasets import load_dataset\n",
      "    >>> ds = load_dataset('imagefolder', data_dir='/path/to/images', split='train')\n",
      "    ```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(load_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970399ac-ffb0-4225-b9e9-4704109acf9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4dc9675-4a33-42e1-bc26-e25950730f0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
